<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>equipo5.js</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/black.css" id="theme">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">
		
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>Equipo 5: Método de máxima verosimilitud como problema de optimización.</section>
				<section>Introducción
					<section>
						En estadística, uno de los principales problemas es determinar el valor de un parámetro (o los valores de varios parámetros) 
						alternativos. 
					</section>
					<section>
						En este sentido, existen diversas metodologías que nos permiten calcular estimadores distintos 
						de un mismo parámetro para una población, tales como el método de momentos y el método de máxima verosimilitud. 
					</section>
					<section>
						Esencialmente, dichas metodologías realizan un proceso de optimización, por lo que naturalmente podemos adecuar 
						este proceso para analizarlo a través de los métodos de descenso en gradiente y Newton-Raphson.
					</section>
				</section>
				<section>Máxima verosimilitud para distribución normal
					<section>
						La distribución de probabilidad de una normal tiene la siguiente forma:

						$$f_x(x|\mu,\sigma)=\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2}\big(\frac{x-\mu}{\sigma}\big)^2}$$
					</section>
					<section>
						Para un conjunto de datos independientes e idénticamente distribuidos, se puede estimar los parámetros $\mu$ y $\sigma$ desconocidos utilizando el método de máxima verosimilitud  
						$$X_n=x_1,x_2,...,x_n \sim N(\mu,\sigma)$$
					</section>
					<section>
						Sea $\mathcal{L}(\mu,\sigma|X_n)$ la función de verosimilitud de la distribución normal. Se tiene que: 

						$$\mathcal{L}(\mu,\sigma|X_n)=\prod^n_{i=1}f_x(x_i|\mu,\sigma)$$


						$$\mathcal{L}(\mu,\sigma|X_n)=(2\pi\sigma^2)^{\frac{-n}{2}} exp\big({-\frac{1}{2\sigma^2}\sum^n_{i=1}(x_i-\mu)^2}\big)$$
					</section>
					<section>
						
						Transformación logarítmica:
						$$ln(\mathcal{L}(\mu,\sigma|X_n))=-\frac{n}{2}ln(2\pi)-\frac{n}{2}ln(\sigma^2) -{\frac{1}{2\sigma^2}\sum^n_{i=1}(x_i-\mu)^2}$$
					</section>
					<section>

						El problema de maximizacion está dado por:
						$$max_{\mu,\sigma}ln(\mathcal{L}(\mu,\sigma|X_n))=
						\\
						-\frac{n}{2}ln(2\pi)-\frac{n}{2}ln(\sigma^2) -{\frac{1}{2\sigma^2}\sum^n_{i=1}(x_i-\mu)^2}$$

					</section>
					<section>
						
						Se cumple que $\mu^*$ y $\sigma^*$ son puntos óptimos si:

						condición necesaria de primer orden:
						$$\nabla(\mathcal{L}(\mu^*,\sigma^*|X_n))=0$$

					</section>
					<section>
						condición suficiente de segundo orden:
	
						$$\nabla^2(\mathcal{L}(\mu^*,\sigma^*|X_n))\in -\mathbb{S}_{++}$$

						o equivalentemente:

						$$-\nabla^2(\mathcal{L}(\mu^*,\sigma^*|X_n))\in \mathbb{S}_{++}$$
					</section>
					<section>
						
							Para esta función de log-verosimilitud, el gradiente está dado por: 



							$$
							\nabla(\mathcal{L}(\mu,\sigma|X_n))=
							\begin{bmatrix}
							\frac{\partial\mathcal{L}(\mu,\sigma|X_n)}{\partial\mu}
							\\
							\\
							\frac{\partial\mathcal{L}(\mu,\sigma|X_n)}{\partial\sigma}

							\end{bmatrix}
							=
							\begin{bmatrix}
							{\frac{1}{\sigma^2}\sum^n_{i=1}(x_i-\mu)}
							\\
							\\
							-\frac{n}{\sigma}+\frac{1}{\sigma^3}\sum^n_{i=1}(x_i-\mu)^2

							\end{bmatrix}
							$$

					</section>
					<section>
						
							Para esta función de log-verosimilitud, la hessiana está dada por: 

								
							$$
							\nabla^2(\mathcal{L}(\mu,\sigma|X_n))=
							\\
							\begin{bmatrix}
							-\frac{1}{\sigma^2}n& -2/ \sigma^3 \sum^n_{​​​​i=1}​​​​(x_i-\mu)
							\\
							\\
							-2/ \sigma^3​​​​ \sum^n_{​​​​i=1}​​​​(x_i-\mu)& \frac{​​​​n}{\sigma^2}-\frac{​​​​4}{\sigma^4}\sum^n_{​​​​i=1}(x_i-\mu)^2

							\end{bmatrix}
							$$

				</section>
					<section>
												
						Para resolver el problema de forma analítica,se tiene un sistema de ecuaciones no lineales con dos incógnitas que están dadas por la condición necesaria de primer orden.

						

							$${\frac{1}{\sigma^2}\sum^n_{i=1}(x_i-\mu)}=0$$
			
							$$-\frac{n}{\sigma}+\frac{1}{\sigma^3}\sum^n_{i=1}(x_i-\mu)^2=0$$
						
					</section>
					<section>

						Resolviendo dicho sistema, tenemos que los estimadores de máxima verosimilitud son:

						$$\mu_{mle}=\frac{1}{n}\sum^n_{i=1}x_i$$

						$$\sigma_{mle}=\sqrt{\frac{1}{n}\sum^n_{i=1}(x_i-\mu_{mle})^2}$$
					</section>

					<section>
						Cuasiconcavidad

						 <img src="images/Captura.PNG"  height="500" width="700">
					</section>

				</section>
				<section>Aplicación Máxima Verosimilitud
					<section>
						Para la estimación de parámetros por el método de máxima verosimilitud, se desarrolló una serie de algoritmos que implementan métodos numéricos necesarios para resolver el problema. 
						
					</section>
					<section>
						Para ilustrar el funcionamiento de estos métodos numéricos, se utiliza una base de datos de rendimiento de trigo por unidad de tierra.
						Esta base de datos es de un estudio históricamente relevante por Mercer and Hall en Rothamsted Experimental Station en Gran Bretaña en el año de 1910.
						Esta estación experimental se fundó en 1834 con el objetivo de experimentar con diferentes cultivos y analizar su rendimiento contrastando el uso de distintos fertilizantes.
					</section>
					<section>

						Histograma de Frecuencias
						<img src="images/Captura2.PNG"  height="500" width="700">
					</section>
					<section> 
						Log Similitud
						<pre><code data-trim data-noescape>
							
							def normal_loglikelihood(params,data):
								mu = params[0]
								sigma = params[1]
								x = data
								n=len(data)

								loglikelihood=-(n/2)*np.log(2*np.pi)-(n/2)*
									np.log(sigma**2)-(1/(2*sigma**2))*np.sum((x-mu)**2)

								return -1*loglikelihood
						</code></pre>
					</section>
					<section> 
						Gradiente
						<pre><code data-trim data-noescape>
							
							def gradient_normal_loglike(params,data):
								mu = params[0]
								sigma = params[1]
								x=data
								n=len(data)
								dmu= (1/(sigma**2))*np.sum(x-mu)
								dsigma=-(n/2)*((2*sigma)/sigma**2)+(1/sigma**3)*np.sum((x-mu)**2)
								return -1*np.array([dmu,dsigma])
						</code></pre>
					</section>
					<section> 
						Descenso en Gradiente
						<pre><code data-trim data-noescape>
							def gradient_descent(x,gradient,data,tol=.0001,maxiter=10000,step_size=.0001):
								x_old=x+10
								i=0
								points=list()
								while ((np.linalg.norm(x-x_old)>=tol)):
									if i==1000:
										break
									points.append(x)
									x_old=x
									x=x-step_size*gradient(x,data)
									i+=1
								return x
						</code></pre>
					</section>
					<section>
						Trayectoria de Descenso
						<img src="images/Captura3.PNG"  height="500" width="700">
					</section>
					<section>
						Resultados
						
						
							<table>
								<thead><tr>
									<th></th>
									<th>$\mu$</th>
									<th>$\sigma$</th>
								</tr></thead>
								<tbody><tr>
									<td>Solución por DG</td>
									<td>3.94832449</td>
									<td>0.45782199</td>
								</tr>
								<tr>
									<td>Solución Analítica </td>
									<td>3.9486399 </td>
									<td>0.45782108</td>
								</tr></tbody>
								<tr>
									<td> Error Relativo</td>
									<td>-7.99e-05</td>
									<td>1.97e-06</td>
								</tr></tbody>
							</table>
						
					</section>



				</section>
				<section>
					Conclusiones:
					<section>
						El método de máxima verosimilitud es una herramienta que nos permite obtener estimadores puntuales 
						de una distribución dada una muestra de datos. Dado que este método implica un proceso de maximización, 
						su implementación depende de métodos numéricos que permitan encontrar los valores máximos de la función. 
					</section>

					<section>
						En este caso, se realizó la implementación en una distribución Normal y un modelo de regresión Poisson. 
						En ambos casos, los estimadores obtenidos utilizando el método de descenso en gradiente y de Newton Raphson 
						son muy similares a los resultados obtenidos de forma analítica.
					</section>
					<section>
						1. La implementación y estimación de parámetros, se puede generalizar para cualquier distribución estadística.
						

						2. Los paquetes estadísticos más utilizados como: statmodels, linalg, etc. utilizan métodos de descenso 
						en la optimización debido a la naturaleza algorítmica. 
					</section>
					<section>
						3. En particular, para el método de newton-raphson, existe una complejidad para la generalización del método 
						de optimización para dimensiones superiores.
						
						
						4. Para poder acelerar la convergencia de los métodos se pueden adicionar metodologías como backtracking 
						y vía exacta en las iteraciones.
					</section>
				</section>	




				
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, Reve ]
			});
		</script>
		<script src="plugin/math/math.js"></script>
		<script>
			Reveal.initialize({
				math: {
				mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
				config: 'TeX-AMS_HTML-full',
				// pass other options into `MathJax.Hub.Config()`
				TeX: { Macros: { RR: "{\\bf R}" } }
				},
				plugins: [ RevealMath ]
			});
		</script>
	</body>
</html>
